{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GRU 2 layers - w2v+pos with scaled dot-product attention.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QBA4_ug9qiBq"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eX4ULnLDcLp0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from google.colab import drive\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "import itertools\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ_izBHQnSyg",
        "colab_type": "text"
      },
      "source": [
        "#Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6I3NSk3nSoH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.read_csv('https://drive.google.com/uc?id=17s-v7RkT7LTojqDDGkVa1dbi6GhukUu6', encoding = \"ISO-8859-1\")\n",
        "val_df = pd.read_csv('https://drive.google.com/uc?id=1afH0fbRM8w9N41R0o9WHHvr1nEso8UEb', encoding = \"ISO-8859-1\")\n",
        "test_df = pd.read_csv('https://drive.google.com/uc?id=1CRQv7ojJG0wSxaRZiotXo4Dwcb2ChKDH', encoding = \"ISO-8859-1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2M7cSH-nSkd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sen_list = train_df['Sentence'].tolist()\n",
        "sen_list_val = val_df['Sentence'].tolist()\n",
        "tag_list = train_df['NER'].tolist()\n",
        "tag_list_val = val_df['NER'].tolist()\n",
        "sen_list_test = test_df['Sentence'].tolist()\n",
        "n_data = train_df.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90P5nfz6nSgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pre_process(sent_list):\n",
        "    output = []\n",
        "    for sent in sent_list:\n",
        "        output.append(sent.split())\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru2MEM_StaNA",
        "colab_type": "text"
      },
      "source": [
        "#Pre-processing for GRU Encoder-Decoder with Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CBpUYtBM3in",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_token_list = pre_process(sen_list)\n",
        "answer_token_list = pre_process(tag_list)\n",
        "input_token_list_val = pre_process(sen_list_val)\n",
        "total_token_list = pre_process(sen_list)\n",
        "total_token_list.extend(input_token_list_val)\n",
        "answer_token_list_val = pre_process(tag_list_val)\n",
        "output_token_list = [[\"<BOS>\"] + s for s in answer_token_list]\n",
        "target_token_list = [s + [\"<EOS>\"] for s in answer_token_list]\n",
        "total_token_list = total_token_list + output_token_list\n",
        "input_token_list_test = pre_process(sen_list_test) #Pre-processing here, only to consider it while establishing the max_length variable due to memory constraints of Colab.\n",
        "total_token_list = total_token_list + input_token_list_test  #Here, we are adding words from test data to use in word_to_ix function so that OOV words don't appear while putting in test data.\n",
        "tot = list(itertools.chain.from_iterable(total_token_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_aCOXuPnSel",
        "colab_type": "code",
        "outputId": "90583caa-b2be-4df0-c7ee-f745c5c52bf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "MAX_LENGTH = max([len(s) for s in total_token_list] + [len(s) for s in target_token_list] + [len(s) for s in input_token_list_test])\n",
        "MAX_LENGTH"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmpaUEDZQvi0",
        "colab_type": "text"
      },
      "source": [
        "##Converting word and data to ix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4r0FITNnSce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_ix = {\"<BOS>\":0,\"<EOS>\":1}\n",
        "\n",
        "for word in tot:\n",
        "  if word not in word_to_ix:\n",
        "    word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "word_list = list(word_to_ix.keys())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kLqNVPOnSYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_index(data, to_ix):\n",
        "    input_index_list = []\n",
        "    for sent in data:\n",
        "        input_index_list.append([to_ix[w] for w in sent])\n",
        "    return input_index_list\n",
        "\n",
        "input_index = to_index(input_token_list, word_to_ix)\n",
        "output_index = to_index(output_token_list, word_to_ix)\n",
        "target_index = to_index(target_token_list, word_to_ix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ewd271P9tha",
        "colab_type": "text"
      },
      "source": [
        "#Input embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dt1igRIu93BZ",
        "colab_type": "text"
      },
      "source": [
        "##Word embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNBIA2DuaLyU",
        "colab_type": "code",
        "outputId": "42959816-f115-409e-cfc2-098503176c71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "import gensim.downloader as api\n",
        "word_emb_model = api.load(\"glove-twitter-25\") \n",
        "\n",
        "embedding_size = 25\n",
        "\n",
        "embedding_matrix = []\n",
        "for word in word_list:\n",
        "    try:\n",
        "        embedding_matrix.append(word_emb_model.wv[word])\n",
        "    except:\n",
        "        embedding_matrix.append([0]*embedding_size)\n",
        "embedding_matrix = np.array(embedding_matrix)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwBwK4OH95jy",
        "colab_type": "text"
      },
      "source": [
        "##PoS tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIZOglT197NL",
        "colab_type": "code",
        "outputId": "4b916012-4589-420c-af85-b7c2b1300a6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tag.perceptron import PerceptronTagger\n",
        "\n",
        "pretrain = PerceptronTagger()\n",
        "POS_mat=pretrain.tag(word_list)\n",
        "POS_mat=np.array(POS_mat)\n",
        "diff_pos=[]\n",
        "for i in range(13972):\n",
        "  if(POS_mat[i][1] not in diff_pos):\n",
        "    diff_pos.append(POS_mat[i,1])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QxA8HEq-J60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "onehotencoder = OneHotEncoder()\n",
        "\n",
        "X = onehotencoder.fit_transform(POS_mat[:,1].reshape(-1,1)).toarray() #reshape the 1-D country array to 2-D as fit_transform expects 2-D and finally fit the object \n",
        "pos_matrix=pd.DataFrame(X) #To add this back into the original dataframe "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKbTrkRO-eaY",
        "colab_type": "text"
      },
      "source": [
        "##Final embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeEl56j3-ayr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emb_mat_df=pd.DataFrame(embedding_matrix)\n",
        "merged_mat= pd.concat((emb_mat_df,pos_matrix), axis=1)\n",
        "\n",
        "merged_matrix=np.array(merged_mat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azqeoULovAbf",
        "colab_type": "text"
      },
      "source": [
        "#Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW10JuhTpEig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, embedding):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = embedding\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.gru2 = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        output, hidden = self.gru2(output,hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMRJsy0qvB-v",
        "colab_type": "text"
      },
      "source": [
        "#Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoKhWAkRpEn3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    ATTN_TYPE_DOT_PRODUCT = \"Dot Product\"\n",
        "    ATTN_TYPE_SCALE_DOT_PRODUCT = \"Scale Dot Product\"\n",
        "\n",
        "    def __init__(self, hidden_size, output_size, embedding, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = embedding\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.gru2 = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size*2, self.output_size)\n",
        "\n",
        "\n",
        "    def cal_attention(self, hidden, encoder_hiddens, method):\n",
        "        if method == AttnDecoderRNN.ATTN_TYPE_DOT_PRODUCT:\n",
        "            attn_weights = F.softmax(torch.bmm(hidden, encoder_hiddens.T.unsqueeze(0)),dim=-1)\n",
        "            attn_output = torch.bmm(attn_weights, encoder_hiddens.unsqueeze(0))\n",
        "            concat_output = torch.cat((attn_output[0], hidden[0]), 1)\n",
        "\n",
        "        elif method == AttnDecoderRNN.ATTN_TYPE_SCALE_DOT_PRODUCT:\n",
        "            attn_weights = F.softmax(1/np.sqrt(hidden_size)*torch.bmm(hidden, encoder_hiddens.T.unsqueeze(0)),dim=-1)\n",
        "            attn_output = torch.bmm(attn_weights, encoder_hiddens.unsqueeze(0))\n",
        "            concat_output = torch.cat((attn_output[0], hidden[0]), 1)\n",
        "\n",
        "        return concat_output\n",
        "\n",
        "    def forward(self, input, hidden, encoder_hiddens):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "        emb, hidden = self.gru(embedded, hidden)\n",
        "        _, hidden = self.gru2(emb, hidden)\n",
        "        \n",
        "        concat_output = self.cal_attention(hidden, encoder_hiddens, AttnDecoderRNN.ATTN_TYPE_SCALE_DOT_PRODUCT)\n",
        "\n",
        "        output = F.log_softmax(self.out(concat_output), dim=1)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tt1QoV7vD-7",
        "colab_type": "text"
      },
      "source": [
        "#Train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHUTqxMupEv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_hiddens = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for i in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)\n",
        "        encoder_hiddens[i] = encoder_hidden[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[0]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    # Teacher forcing: Feed the target as the next input\n",
        "    for i in range(target_length):\n",
        "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_hiddens)\n",
        "        loss += criterion(decoder_output, target_tensor[i])\n",
        "        decoder_input = target_tensor[i]  # Teacher forcing\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kubLFQhu9pU",
        "colab_type": "text"
      },
      "source": [
        "#Evaluate function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBIiL70Mu8wG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_sent = pre_process([sentence])[0]\n",
        "        intput_index = [word_to_ix[word] for word in input_sent]\n",
        "        input_tensor = torch.LongTensor([[ind] for ind in intput_index]).to(device)\n",
        "\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_hiddens = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "            encoder_hiddens[ei] += encoder_hidden[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[0]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_hiddens)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == 1:\n",
        "                #decoded_words.append('<EOS>')\n",
        "                break\n",
        "            #elif len(decoded_words) == len(input_tensor[ei]):\n",
        "                #break\n",
        "            else:\n",
        "                decoded_words.append(word_list[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-mW8KMUwqB1",
        "colab_type": "text"
      },
      "source": [
        "#Validation accuracy function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbgkLlgCwpCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "val_true_tags = [word for sen in tag_list_val for word in sen]\n",
        "test_words_input = val_df['Sentence'].tolist()\n",
        "gr_tags = [i.split() for i in tag_list_val]\n",
        "\n",
        "def calc_acc(pred,actual):\n",
        "  s = 0\n",
        "  pred_tags=[]\n",
        "  for i in range(0,len(pred)):\n",
        "    if len(pred[i]) < len(actual[i]):\n",
        "      pre = ['O'] * (len(actual[i])-len(pred[i]))\n",
        "      pre1 = pred[i]\n",
        "      pre1.extend(pre)\n",
        "      s += accuracy_score(pre1,actual[i])\n",
        "      pred_tags.append(pre1)\n",
        "    else:\n",
        "      pre = pred[i][:len(actual[i])]\n",
        "      s += accuracy_score(pre,actual[i])\n",
        "      pred_tags.append(pre)\n",
        "  return (s/len(pred)), pred_tags"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNrcK6zCvkni",
        "colab_type": "text"
      },
      "source": [
        "#Train-Iterations function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KTv5nxlpE7R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "def trainIters(encoder, decoder, n_iters, print_every=1000, learning_rate=0.01):\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    \n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        random_choice_ix = random.choice(range(n_data))\n",
        "        input_index_r = [[ind] for ind in input_index[random_choice_ix]]\n",
        "        target_index_r = [[ind] for ind in target_index[random_choice_ix]]\n",
        "        \n",
        "        input_tensor = torch.LongTensor(input_index_r).to(device)\n",
        "        target_tensor = torch.LongTensor(target_index_r).to(device)\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('Epochs - (%d %d%%) %.4f' % (iter, iter / n_iters * 100, print_loss_avg))\n",
        "        \n",
        "\n",
        "        #We started generating val_accuracy after model has trained with some epochs so as to preserve some computation time\n",
        "        val_print = print_every*4\n",
        "        if iter>20000:\n",
        "            if iter % val_print == 0:\n",
        "                val_predicted = []\n",
        "                for i in test_words_input:\n",
        "                    predicted_sen = (evaluate(encoder1, attn_decoder1, i, max_length=MAX_LENGTH))\n",
        "                    val_predicted.append(predicted_sen)\n",
        "                val_acc, val_tags_pred = calc_acc(val_predicted, gr_tags)\n",
        "                print('Epochs - (%d %d%%) Train loss - %.4f Validation accuracy - %.4f' % (iter, iter / n_iters * 100, print_loss_avg, val_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBcQ9N4LvxIK",
        "colab_type": "text"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToSuYy1RgZwh",
        "colab_type": "code",
        "outputId": "4d04ef03-10e8-4592-9402-04375cb19fed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        }
      },
      "source": [
        "input_size = merged_matrix.shape[0]\n",
        "hidden_size = merged_matrix.shape[1]\n",
        "\n",
        "embedding = nn.Embedding(input_size, hidden_size)\n",
        "embedding.weight.data.copy_(torch.from_numpy(merged_matrix))\n",
        "\n",
        "encoder1 = EncoderRNN(input_size, hidden_size, embedding).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, input_size, embedding, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 48000, print_every=1000)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epochs - (1000 2%) 1.3608\n",
            "Epochs - (2000 4%) 0.9279\n",
            "Epochs - (3000 6%) 0.7922\n",
            "Epochs - (4000 8%) 0.7285\n",
            "Epochs - (5000 10%) 0.6494\n",
            "Epochs - (6000 12%) 0.6228\n",
            "Epochs - (7000 14%) 0.6106\n",
            "Epochs - (8000 16%) 0.5596\n",
            "Epochs - (9000 18%) 0.5134\n",
            "Epochs - (10000 20%) 0.5091\n",
            "Epochs - (11000 22%) 0.4954\n",
            "Epochs - (12000 25%) 0.4612\n",
            "Epochs - (13000 27%) 0.4330\n",
            "Epochs - (14000 29%) 0.4274\n",
            "Epochs - (15000 31%) 0.4073\n",
            "Epochs - (16000 33%) 0.3902\n",
            "Epochs - (17000 35%) 0.3627\n",
            "Epochs - (18000 37%) 0.3366\n",
            "Epochs - (19000 39%) 0.3294\n",
            "Epochs - (20000 41%) 0.3273\n",
            "Epochs - (21000 43%) 0.2964\n",
            "Epochs - (22000 45%) 0.2965\n",
            "Epochs - (23000 47%) 0.2736\n",
            "Epochs - (24000 50%) 0.2738\n",
            "Epochs - (24000 50%) Train loss - 0.2738 Validation accuracy - 0.9081\n",
            "Epochs - (25000 52%) 0.2555\n",
            "Epochs - (26000 54%) 0.2542\n",
            "Epochs - (27000 56%) 0.2332\n",
            "Epochs - (28000 58%) 0.2355\n",
            "Epochs - (28000 58%) Train loss - 0.2355 Validation accuracy - 0.8979\n",
            "Epochs - (29000 60%) 0.2443\n",
            "Epochs - (30000 62%) 0.2227\n",
            "Epochs - (31000 64%) 0.1971\n",
            "Epochs - (32000 66%) 0.2219\n",
            "Epochs - (32000 66%) Train loss - 0.2219 Validation accuracy - 0.9018\n",
            "Epochs - (33000 68%) 0.2203\n",
            "Epochs - (34000 70%) 0.2014\n",
            "Epochs - (35000 72%) 0.1794\n",
            "Epochs - (36000 75%) 0.1791\n",
            "Epochs - (36000 75%) Train loss - 0.1791 Validation accuracy - 0.9117\n",
            "Epochs - (37000 77%) 0.1695\n",
            "Epochs - (38000 79%) 0.1663\n",
            "Epochs - (39000 81%) 0.1587\n",
            "Epochs - (40000 83%) 0.1637\n",
            "Epochs - (40000 83%) Train loss - 0.1637 Validation accuracy - 0.9224\n",
            "Epochs - (41000 85%) 0.1719\n",
            "Epochs - (42000 87%) 0.1409\n",
            "Epochs - (43000 89%) 0.1232\n",
            "Epochs - (44000 91%) 0.1438\n",
            "Epochs - (44000 91%) Train loss - 0.1438 Validation accuracy - 0.9228\n",
            "Epochs - (45000 93%) 0.1481\n",
            "Epochs - (46000 95%) 0.1394\n",
            "Epochs - (47000 97%) 0.1298\n",
            "Epochs - (48000 100%) 0.1956\n",
            "Epochs - (48000 100%) Train loss - 0.1956 Validation accuracy - 0.8990\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVQJP76uH7_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}